* speech synthesis

** speech synthesis techniques using deep neural networks

https://medium.com/@saxenauts/speech-synthesis-techniques-using-deep-neural-networks-38699e943861

*** concatenative tts

concatenate audio clips (or units) recordings

- clean, clear, intelligible
- emotionless, unnatural

*** parametric tts

features `vocoder` audio
+ linguistic features (phonemes, duration) from text
+ vocoder features (cepstra, spectrogram, fundamental frequency)
+ estimates parameters (phase, prosody, intonation)

- muffled, buzzing, noisy
- neither intelligible nor natural

*** deep learning

- wavenet
- samplernn
- char2wav
- tacotron

** speech production process

https://medium.com/@Alibaba_Cloud/interspeech-2017-speech-synthesis-technology-890c225d2006

- parametric
- splicing (concatenative)
- waveform-based

*** assessment criteria

- mean opinion score (mos) 1=bad 2=poor 3=fair 4=good 5=excellent
- coustic quality, intelligibility, similarity, holistic naturalness

** music genre recognition

http://deepsound.io/music_genre_recognition.html

** tacotron

https://github.com/keithito/tacotron

- end-to-end, seq-to-seq, seq char -> seq frame
- 256-D character embedding
- 24kHz 2048-point fourier transform

*** spectral analysis

- pre-emphasis 0.97
- frame length 50ms
- frame shift 12.5ms
- window type Hann

*** pre-processing

- text normalization: 16 -> sixteen

*** encoder pre-net

- dense 256
- dropout 0.5
- dense 128
- dropout 0.5

*** CBHG encoder

**** convolution bank

- K=16 sets of 128 1-D convolutional filters (ngrams)
- width 2 max-pooling for local invariances
- stride 1 for preserving the original time resolution
- 2 layer width 3 128 1-D convolution projections
- residual connections
- batch normalization for all convolutional layers

**** highway network

extract high-level features

- 4 dense layers 128 units

**** bidirectional gru

extract sequential features from both forward and backward context

- bidrectional 128 gru

*** decoder pre-net

- dense 256
- dropout 0.5
- dense 128
- dropout 0.5

*** rnn decoder

+ 80-band mel-scale spectrogram
+ r=2 output frames at once

- 2 vertical residual 256 gru
- 1 attention 256 gru

*** loss

loss = mel_loss + linear_loss

mel_loss = tf.reduce_mean(tf.abs(mel_targets - mel_outputs))
linear_loss = 0.5 * tf.reduce_mean(l1) + 0.5 * tf.reduce_mean(l1[:,:,0:n_priority_freq])
l1 = tf.abs(linear_targets - linear_outputs)
n_priority_freq = int(3000 / (hp.sample_rate * 0.5) * hp.num_freq)

*** post-processing CBHG

+ predict spectral magnitude sampled on a linear-frequency scale
+ Griffin-Lim algorithm: spectrogram -> waveform
+ better to have a trainable spectrogram to waveform inverter

- K=8 sets of 128 1-D convolutional filters
- stride 1 width 2 max-pooling
- 2 layer width 3 128 1-D convolution projections
- 4 dense layers 128 units
- bidrectional 128 gru

** tacotron 2

- characters -> spectrogram
- wavenet vocoder: spectrogram -> waveform

*** encoder

- char embedding
- 3 conv layers
- bidirectional lstm

*** decoder

- attention
- 2 lstm layers
- 2 layer pre-net
- and stuff

*** wavenet vocoder

- 30 dilated convolution
- 3 dilation cycles
- 16-bit samples at 24kHz
- 10 component mixture of logistic distributions

*** train

- teacher-forcing

** deep voice

*** grapheme to phoneme

- 3 layer bidirectional 1024 gru encoder
- 3 layer unidirectional 1024 gru decoder
- trained with teacher forcing and 0.95 dropout
- deocding with beam search

*** phoneme boundary detection

- connectionist temporal classification (CTC) loss
- predict phoneme pairs

*** phoneme duration and fundamental frequency prediction

seq phoneme -> duration, p(voiced), f0

- 2 layer dense 256 units
- 2 layer unidirectional 128 gru
- output layer

*** audio synthesis

- wavenet variant

** deep voice 3

*** text preprocessing

- uppercase all
- remove all intermediate punctuation
- end every utterance with a period or question mark
- replace spaces by 4 duration of pauses

*** encoder

- embedding
- dense projection
- fully-convolutional

*** decoder

- fully-convolutional, causal
- multi-hop convolutional attention
- autoregressive
- predict r mel frames

attention

- query vector, per-timestep key vector -> attention weights
- context vector = weighted average of value vectors
- positional encoding to key and query vectors (monotonic progression)

*** converter

- fully-convolutional, non-causal

vocoder

- griffin-lim: l1
- world: xent for voiced prediction, l1 for f0, spectral, aperiodicity
- wavenet: l1 on mel

** char2wav

*** encoder

- brnn

*** decoder

- produces vocoder features
- location-based attention rnn

*** vocoder

- samplernn

** samplernn

- a hierarchy of modules, each operating at a different temporal resolution
- the lowest module processes individual samples
- each higher module operates on an increasingly longer timescale and a lower temporal resolution
- each module conditions the module below it
- with the lowest module outputting sample-level predictions
- the entire hierarchy is trained jointly end-to-end by backpropagation

*** module

- at the k-th level
- non-overlapping frames of FS^k samples
- a deep rnn summarizing the history of its inputs into a conditioning vector for the next module downward
- each timestep is a fixed length hidden state
- upsample the conditioning vector into a series of vectors by linear projections
- the lowest module is a mlp instead of rnn

*** output

- 256-way softmax
- linear quantization
- embed quantizated input

** singing synthesizer

*** model

model features produced by a parametric vocoder that separates the
influence of pitch and timbre, instead of raw waveform

- input: a window of past acoustic features
- output: acoustic features of the current time step

how are the accustic features obtained ????

*** mixture density output

makes frame-wise predictions using mixture density outputs rather than
categorical outputs

- mixture of 4 gaussian components: mean, variance, skewness, shape

*** regularization

regularize the model and make the autoregressive generation process
more robust to prediction errors

- denoising objective
- temperature softmax

*** multi-stream architecture

using a simple multi-stream architecture, harmonic, aperiodic and
voiced/unvoiced components can all be predicted in a coherent manner

** deep voice 2

separate phoneme duration and frequency models

- speaker embedding
- initialized in uniform(-0.1,0.1)
- used in multiple places through projection
  + initial recurrent hidden state
  + concate to the input at every timestep
  + feature gating

** deep speaker

- speaker embedding
- triplet loss

* attention

- x : R^{s,i}
- q : R^{j}
- c = a x : R^{i}
- a = softmax (f q x) : R^{s}
- f : R^{j} -> R^{i} -> R

** additive attention

- f q x = v (tanh (w (q ++ x)))
- w : R^{k,j+i}
- v : R^{k}

** multiplicative attention

- f q x = q w x
- w : R^{j,i}
- q : R^{j}

*** scaled dot-product attention

- f q x = d2^{-1/2} q w x

** self-attention

- f q x = f x
- f x = v (tanh (w x))
- w : R^{k,i}
- v : R^{k}

*** multi-hop attention

- v : R^{n,k}
- f : R^{i} -> R^{n}
- a : R^{n,s}
- c : R^{n,i}

penalize the frobenius-norm of a a^T - I

*** multi-headed attention

TODO

*** key-value attention

TODO

** positional encoding

TODO

** lit

*** [[https://arxiv.org/abs/1409.0473][2014 bahdanau et al.]]
neural machine translation by jointly learning to align and translate

- rnn
- hidden state as query vector
- additive attention

*** [[luong et al.][https://arxiv.org/pdf/1508.04025.pdf]]
effective approaches to attention-based neural machine translation

- multiplicative attention

*** [[https://arxiv.org/abs/1503.08895][2015 sukhbaatar et al.]]
end-to-end memory networks (question answering)

- externel query as query vector

*** [[https://arxiv.org/abs/1601.06733][2016 cheng et al.]]
long short-term memory-networks for machine reading

- lstm
- replace memory cell with memory network
- memory grows with time

*** [[https://arxiv.org/abs/1606.01933][2016 parikh et al.]]
a decomposable attention model for natural language inference

*** [[https://arxiv.org/abs/1609.08144][2016 wu et al.]]
google's neural machine translation system: bridging the gap between human and machine translation

- lstm encoder, bidirectional
- attention
- lstm decoder

*** [[https://arxiv.org/abs/1702.00887][2017 kim et al.]]
structured attention networks

- x = x_1, ..., x_n
- query vector q
- categorical latent variable a over {1,...,n}
- attention distribution a ~ p(a | x,q)
- annotation function f
- context vector c = E_{a ~ p(a | x,q)}[f(x,a)]
- p as crf

*** [[https://arxiv.org/abs/1703.03130][2017 lin et al.]]
a structured self-attentive sentence embedding

- self-attention

*** [[https://arxiv.org/abs/1702.04521][2017 daniluk et al.]]
frustratingly short attention spans in neural language modeling

- key-value attention

*** [[https://arxiv.org/abs/1705.03122][2017 gehring et al.]]
convolutional sequence to sequence learning

- positional encoding

*** [[https://arxiv.org/abs/1705.04304][2017 paulus et al.]]
a deep reinforced model for abstractive summarization

*** [[https://arxiv.org/abs/1706.03762][2017 vaswani et al.]]
attention is all you need

- scaled dot-product attention
- multi-headed attention
